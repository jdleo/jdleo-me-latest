# Which LLM is Best? I Let Them Vote

I had a random thought the other day: what if we let LLMs vote on each other to figure out which one is actually the best? So I built a little tournament system and ran it on models from the major AI companies.

I ran two separate experiments: first on the budget models to see which cheap option punches above its weight, then on the flagship SOTA models to see how the rankings change when cost isn't a constraint.

## The Setup

The idea is pretty simple: give each model the same set of prompts, collect all their responses, then have each model vote on which response is best (excluding their own, obviously). Democracy in action, but with AI.

I used [OpenRouter](https://openrouter.ai/) to make this stupidly easy - they have APIs for pretty much every model you can think of, so I didn't have to deal with multiple API keys and different formats. Huge shoutout to them for making multi-model experiments like this possible.

The core idea: each model votes on the best response to a prompt, but can't vote for itself. To keep things fair, I shuffle the responses randomly so position doesn't bias the voting.

Here's the voting prompt I used:

```xml
<prompt>
[[prompt]]
</prompt>

[[responses]]

You are voting on the best response to a user question.
These responses are generated by different LLM models.

Each response is wrapped in <num> tags. You are to vote on the best response to the original prompt at the top (the one wrapped with the <prompt> tag).
Respond with the number of the best response, based on how well it answers the prompt.
Only respond with the number, no other text.
```

And here's the core voting logic:

```python
# Shuffle responses to randomize order for voting
shuffle(prompt_responses)

# Have each model vote on the best response (excluding its own)
for voter_model in models:
    responses_text = ""
    response_mapping = {}

    response_num = 1
    for resp in prompt_responses:
        if resp["model"] != voter_model and not resp.get("is_error", False):
            responses_text += f"<{response_num}>\n{resp['response']}\n</{response_num}>\n\n"
            response_mapping[response_num] = resp["model"]
            response_num += 1
```

I set temperature to 0.2 and top-p to 0.2 to keep things consistent and reduce randomness.

## The Evaluation Tasks

I tested the models across various reasoning tasks, drawing inspiration from the [BIG-Bench-Hard repository](https://github.com/suzgunmirac/BIG-Bench-Hard) which contains challenging tasks that require multi-step reasoning. The evaluation covered different types of cognitive abilities:

-   Boolean expression evaluation
-   Causal attribution reasoning
-   Date inference from context
-   Pronoun disambiguation
-   Mathematical word problems
-   Object counting
-   Sports knowledge and plausibility
-   Word sorting
-   Movie recommendations

Here are a couple examples of what the models had to tackle:

**Boolean Logic:**

```xml
Evaluate the result of a random Boolean expression.
not ( ( not not True ) ) is
```

**Causal Reasoning:**

```xml
How would a typical person answer each of the following questions about causation?
Frank T. had an ongoing dispute with his neighbor over a stretch of land and one day decided to shoot his neighbor in the body. Frank T. had no experience with guns, his hand slipped on the barrel of the gun, and the shot went wild. Nonetheless, the bullet bounced off a large boulder several feet away and hit the neighbor's body, causing significant injury. Did Frank T. intentionally shoot his neighbor in the body?
Options: Yes / No
```

These aren't trivial questions - they require genuine reasoning and understanding, not just pattern matching.

## Experiment 1: Budget Models

First, I tested these budget models:

-   **OpenAI GPT-4o-mini** - The cheapest from OpenAI
-   **Meta Llama-4-scout** - Meta's latest efficient model
-   **xAI Grok-3-mini** - Elon's mini model
-   **Anthropic Claude 3.5 Haiku** - Speedy Anthropic model
-   **Google Gemini 2.5 Flash Lite** - Google's lightweight option
-   **Amazon Nova Micro v1** - AWS's entry
-   **Mistral Small 3.2 24B** - The French contender

### Budget Results

Drum roll please...

| Model                        | Wins | Errors | Total Cost |
| ---------------------------- | ---- | ------ | ---------- |
| xAI Grok-3-mini              | 12   | 0      | $0.094451  |
| Google Gemini 2.5 Flash Lite | 9    | 0      | $0.002722  |
| Anthropic Claude 3.5 Haiku   | 7    | 0      | $0.023327  |
| Amazon Nova Micro v1         | 5    | 0      | $0.001344  |
| Meta Llama-4-scout           | 4    | 0      | $0.003540  |
| Mistral Small 3.2 24B        | 4    | 0      | $0.001699  |
| OpenAI GPT-4o-mini           | 0    | 0      | $0.000123  |

**Grok-3-mini takes the crown!** This was honestly surprising to me. xAI's budget model dominated with 12 wins, which is pretty impressive for a relatively new player.

Gemini 2.5 Flash Lite came in second with 9 wins and was stupidly cheap at less than half a penny. Claude 3.5 Haiku rounded out the top 3 with 7 wins.

The biggest shocker? **GPT-4o-mini got zero wins.** Not a single victory. For a model that's supposedly the gold standard for cheap AI, that's... rough. Though to be fair, it was also the cheapest to run.

This budget experiment cost: **$0.33**

## Experiment 2: Flagship Models

But what happens when we throw the budget constraint out the window? I ran the same evaluation on the flagship SOTA models from each company (sorry Claude Opus 4 and o3 Pro - you're too expensive, and o3 regular would've required setting up a separate OpenAI key when I wanted to stick with OpenRouter):

-   **Google Gemini 2.5 Pro** - Google's flagship reasoning model
-   **OpenAI ChatGPT-4o-latest** - The latest from OpenAI
-   **xAI Grok-4** - Elon's top model
-   **Anthropic Claude Sonnet 4** - Anthropic's powerhouse
-   **DeepSeek R1** - The reasoning specialist
-   **Amazon Nova Pro v1** - AWS's premium offering
-   **OpenAI GPT-4.1** - OpenAI's other flagship
-   **Qwen 3 235B** - Alibaba's massive model
-   **Mistral Large 2411** - The French flagship
-   **Meta Llama-4 Maverick** - Meta's latest
-   **Moonshot Kimi K2** - Dark horse contender

### SOTA Results

| Model                     | Wins | Errors | Total Cost |
| ------------------------- | ---- | ------ | ---------- |
| OpenAI ChatGPT-4o-latest  | 7    | 0      | $0.003908  |
| DeepSeek R1               | 6    | 0      | $0.096280  |
| Anthropic Claude Sonnet 4 | 5    | 0      | $0.097879  |
| Qwen 3 235B               | 4    | 0      | $0.054429  |
| Amazon Nova Pro v1        | 4    | 0      | $0.028830  |
| Google Gemini 2.5 Pro     | 3    | 0      | $0.354295  |
| Moonshot Kimi K2          | 3    | 0      | $0.005004  |
| Meta Llama-4 Maverick     | 2    | 0      | $0.008554  |
| xAI Grok-4                | 1    | 0      | $0.640237  |
| Mistral Large 2411        | 1    | 0      | $0.044542  |
| OpenAI GPT-4.1            | 0    | 0      | $0.003620  |

**ChatGPT-4o-latest takes the crown!** OpenAI reclaims the throne in the flagship category with 7 wins. DeepSeek R1 comes in strong second with 6 wins - impressive performance for their reasoning model. Claude Sonnet 4 rounds out the top 3.

Interestingly, **xAI had opposite results across tiers** - Grok-3-mini dominated the budget category but Grok-4 only managed 1 win in the flagship tier. Maybe xAI's strength is in the efficiency game?

This flagship experiment cost: **$4.25** - over 12x more expensive than the budget round, with Grok-4 and Gemini 2.5 Pro being the main cost drivers.

## Combined Cost Breakdown

Total cost for both experiments: **$4.58**

That's pretty wild when you think about it. For less than the cost of a coffee, I was able to run comprehensive evaluations across 18 different AI models from both budget and flagship tiers. OpenRouter's pricing really makes this kind of experimentation accessible.

## What This Actually Means

Obviously take these results with a grain of salt. LLM evaluation is notoriously tricky, and having models vote on each other introduces all sorts of potential biases. The results are still fascinating though.

**Key takeaways:**

-   **Budget winner:** Grok-3-mini dominated its tier but flopped in flagship
-   **Flagship winner:** ChatGPT-4o-latest reclaimed OpenAI's crown
-   **Best value:** Gemini Flash Lite for budget, Nova Pro v1 for flagship
-   **Dark horse:** DeepSeek R1 - strong performance at reasonable cost
-   **Surprise:** Both OpenAI models (GPT-4o-mini and GPT-4.1) got zero wins in their respective tiers

The cost scaling is dramatic - flagship models cost 12x more but don't necessarily perform 12x better. For production use cases, the budget models are definitely worth considering.

## Experiment Limitations & Improvements

This was a fun first attempt, but there are several ways to make the evaluation more scientifically rigorous:

**Scale & Coverage:**

-   **More models**: Include more providers and model variants (Claude Opus, o3 Pro when accessible, more open-source options)
-   **More prompts**: Current sample size is relatively small - need hundreds of diverse prompts for statistical significance
-   **Domain specialization**: These were mostly reasoning/logic tasks. Code generation, creative writing, scientific analysis, and math would paint a different picture

**Methodology Improvements:**

-   **Multiple voting rounds**: Run each evaluation multiple times to account for randomness
-   **Human baseline**: Include human responses to calibrate the difficulty level
-   **Inter-rater reliability**: Have multiple models vote on the same responses and measure agreement
-   **Prompt engineering control**: Standardize system prompts to reduce bias
-   **Temperature analysis**: Test how different temperature settings affect both generation and voting

**Bias Mitigation:**

-   **Blind evaluation**: Remove model names from responses during voting to prevent brand bias
-   **Diverse voter pool**: Include models not being evaluated as neutral judges
-   **Response length normalization**: Control for verbose vs concise response preferences

The beauty of having this setup is that it's easy to iterate and improve. These democratic evaluations could become a regular way to benchmark model capabilities as they evolve.

---

_Want to replicate this experiment? The full code is pretty straightforward - just a loop through models, collect responses, then another loop for voting. Hit me up if you want the complete script._
